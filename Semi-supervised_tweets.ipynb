{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words as vocab_words\n",
    "from functools import partial\n",
    "from collections.abc import Iterable\n",
    "from ast import literal_eval\n",
    "import string\n",
    "import os \n",
    "import enchant\n",
    "import numpy as np\n",
    "import gc\n",
    "stop_words = stopwords.words('english')\n",
    "dictionary = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAPREPROCESSING HYPERPARAMETERS\n",
    "RAW_DATASET = \"./raw_dataset/twcs/twcs.csv\"\n",
    "PROCESSED_DATASET = \"./processed_dataset\"\n",
    "CORES = cpu_count()\n",
    "DATA_CHUNK = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def clean_up(self, interest_columns):\n",
    "        # Check if any entry in any of the columns is Null,NaN and NaT.\n",
    "        print(\"Check for Null, NaN, NaT ...\")\n",
    "        for col in interest_columns:\n",
    "            if self.df[col].isnull().values.any():\n",
    "                print(f\"Values missing in the column : {col}\")\n",
    "                print(f\"Elements to be dropped from the from the column : {col}\")\n",
    "            else:\n",
    "                print(f\"{col} is clean\")\n",
    "        \n",
    "        self.df.dropna()\n",
    "        print(\"\\nCheck for duplicate entries ...\")\n",
    "        for col in interest_columns:\n",
    "            if self.df[col].duplicated().any():\n",
    "                print(f\"Duplicate values in the column : {col}\")\n",
    "                print(\"Dropping duplicates\")\n",
    "                self.df[col].drop_duplicates()\n",
    "            else:\n",
    "                print(f\"{col} is clean\")\n",
    "\n",
    "    def split_df(self, split_ratio=[0.6,0.2]):\n",
    "        # Split the df into train, val and test sets\n",
    "        # sets = [train, val and test]\n",
    "        sets = np.split(self.df.sample(frac=1), [int(split_ratio[0]*len(self.df)), int((1-split_ratio[1])*len(self.df))])\n",
    "        del self.df\n",
    "        return sets\n",
    "\n",
    "    # To have a generalized column dropping, it can be generalized by passing drop column as an argument.\n",
    "    def drop_cols(self, sets, drop_columns, index=None):\n",
    "        processed_sets = []\n",
    "        for sub_set in sets:\n",
    "            sub_set = sub_set.drop(columns=drop_columns)\n",
    "            if index is not None:\n",
    "                sub_set = sub_set.set_index(index)\n",
    "            processed_sets.append(sub_set)\n",
    "        return processed_sets\n",
    "\n",
    "    # Tokenize and Process the text column\n",
    "    def tokenize_tweets(self, text_column, sub_set):\n",
    "        sub_set['Processed_text'] = sub_set[text_column].apply(lambda tweet: word_tokenize(tweet)).apply(lambda tweet : [re.sub('[0-9]', '', i) for i in tweet])\n",
    "        sub_set['Processed_text'] = sub_set['Processed_text'].apply(lambda tweet: [re.sub('(?<=[a-z])\\'(?=[a-z])', '', i) for i in tweet])\n",
    "        sub_set['Processed_text'] = sub_set['Processed_text'].apply(lambda tweet: [word for word in tweet if len(word) > 0])\n",
    "        sub_set['Processed_text'] = sub_set['Processed_text'].apply(lambda tweet: [word for word in tweet if dictionary.check(word)])\n",
    "        sub_set['Processed_text'] = sub_set['Processed_text'].apply(lambda tweet :[word.lower() for word in tweet if word not in string.punctuation])\n",
    "        sub_set['Processed_text'] = sub_set[\"Processed_text\"].apply(lambda tokens : [token for token in tokens if not isinstance(token, (int, float))])\n",
    "        sub_set = sub_set[sub_set[\"Processed_text\"].str.len()>1]\n",
    "        return sub_set\n",
    "    \n",
    "# Parallelized multiprocessing of the dataframes\n",
    "def parallelized_process(df_set, func, col_name=None, num_cores=1):\n",
    "    final_set = []\n",
    "    process_id = 0\n",
    "    for st in df_set:\n",
    "        divide_df = np.array_split(st, num_cores)\n",
    "        if col_name is None:\n",
    "            pool_obj= Pool(num_cores)\n",
    "            st_join = pd.concat(pool_obj.map(func, divide_df))\n",
    "            pool_obj.close()\n",
    "            pool_obj.join()\n",
    "        else:\n",
    "            operate_func = partial(func, col_name)\n",
    "            pool_obj= Pool(num_cores)\n",
    "            st_join = pd.concat(pool_obj.map(operate_func, divide_df))\n",
    "            pool_obj.close()\n",
    "            pool_obj.join()\n",
    "        final_set.append(st_join)\n",
    "    \n",
    "    return final_set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "* To enable switch the change value to 1 in the if condition\n",
    "* The Preprocessing of the dataset\n",
    "* Setting values to the below parameters require a basic understanding of the dataset, which columns are relevant\n",
    "* Considering the use-case of sentiment Analysis, The preprocessing component is dataset agnostic\n",
    "\"\"\"\n",
    "# Configuration Parameters\n",
    "INTEREST_COLS = [\"tweet_id\", \"text\"]\n",
    "DROP_COLS = [\"author_id\", \"inbound\", \"created_at\", \"response_tweet_id\", \"in_response_to_tweet_id\"]\n",
    "TEXT_COL = \"text\"\n",
    "\n",
    "\n",
    "if(0):\n",
    "    df_chunks = pd.read_csv(RAW_DATASET, delimiter=\",\", encoding=\"\",chunksize= 10000)\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    test_list = []\n",
    "    for i, chunk in enumerate(df_chunks):\n",
    "        print(f\"\\n\\nProcessing chunk : {i+1} ...\")\n",
    "        preprocess = Preprocessing(chunk)\n",
    "        preprocess.clean_up(INTEREST_COLS)\n",
    "        sets = preprocess.split_df()\n",
    "        train, val, test = preprocess.drop_cols(sets, DROP_COLS)\n",
    "        train, val = parallelized_process([train, val], preprocess.tokenize_tweets, col_name=TEXT_COL, num_cores=CORES)\n",
    "        train_list.append(train)\n",
    "        val_list.append(val)\n",
    "        test_list.append(test)\n",
    "\n",
    "    # Create processed sets\n",
    "    processed_sets = [pd.concat(train_list), pd.concat(val_list), pd.concat(test_list)]\n",
    "    # Store to the system\n",
    "    [st.to_csv(os.path.join(PROCESSED_DATASET, f\"{name}.csv\")) for name, st in zip([\"train_set\", \"val_set\", \"test_set\"], processed_sets)]\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Transformer based architecture chosen ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation dimension\n",
    "\n",
    "* The current state of the art network on Stanford Standard Treebank(SST-2) is ALBERT which is self-supervised learnt language representation model based on Bidirectional Encoder Representation from Transformer.\n",
    "\n",
    "paper link : https://arxiv.org/pdf/1909.11942.pdf\n",
    "\n",
    "github repo : https://github.com/huggingface/transformers (open sourced, under Apache License 2.0, a python pip package transformers)\n",
    "\n",
    "* The model achieves 97.4% Accuracy on SST-2 and therefore a right candidate to be transfer learnt or used for label generation for domain specific datasets.\n",
    "\n",
    "\n",
    "### Algorithmic dimension\n",
    "\n",
    "\n",
    "* A constant time sequential operation.\n",
    "\n",
    "* Degree one polynomial vector dimension proportionlity for per layer complexity.\n",
    "\n",
    "* Linear network propagation path length\n",
    "\n",
    "* The core idea behind the model is the usage of the bidirectional attention mechanism to learn the dependencies between the latent vector representations.\n",
    "\n",
    "* All vectors are fed to the network parallely rather sequentially, there is no sequential dependence and there is no backpropagation through time unlike RNNs.\n",
    "\n",
    "* The attention mechanism allow connections between the hidden states (output vector) of each encoder and decoder, so that the target word is predicted based on a combination of vectors, rather than just the hidden state of the decoder, this mechanism gives the decoder access to all the hidden states of the encoder.\n",
    "\n",
    "* A weighted hidden state tensor parameterized by the attention is fed to the decoder, to make prediction about the next vector state. There is no use of memory here, compared to the RNNs.\n",
    "\n",
    "* Overall, transformer based architecture beats RNN, CNN based models on SST-2 by a huge margin, as seen in the first plot. The best RNN based accuracy on SST-2 achieved is 93.200 by Block-sparse LSTM while state of the art is 97.40%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "* Generating label for the unlabelled tweets for training using the transformer based Bert model\n",
    "* loading the train and validation sets in chunks, instantiating the transformer model and run a forward pass\n",
    "over each tweet to generate prediction. There are two predictions:\n",
    "    label : Positive / Negative \n",
    "    score : The probability, how strong/confident the label is. (0-1)\n",
    "    For e.g. label = \"Negative\" and score = 0.9932239234 -> Strongly negative tweet\n",
    "\n",
    "'''\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def gen_sentiment(model, df_slice):\n",
    "    for row in range(len(df_slice)):\n",
    "        if type(df_slice.iloc[row, 3]) is not str:\n",
    "            df_slice.iloc[row, 3] = str(df_slice.iloc[row, 3])\n",
    "            df_slice.iloc[row, 3] = df_slice.iloc[row,3][1:-2]\n",
    "        else:\n",
    "            df_slice.iloc[row, 3] = df_slice.iloc[row,3][1:-2].replace(\"'\", \"\").replace(\",\", \"\")\n",
    "    list_tweets = [df_slice.iloc[row, 3] for row in range(len(df_slice))]\n",
    "    df_slice['label'] = [predict[\"label\"] for predict in model(list_tweets)]\n",
    "    df_slice['score'] = [predict[\"score\"] for predict in model(list_tweets)]\n",
    "    df_slice[\"label\"] = df_slice[\"label\"].apply(lambda label: 0 if label == 'NEGATIVE' else 1 )\n",
    "    df_slice = df_slice.drop(['Unnamed: 0', 'tweet_id', 'text'], axis=1)\n",
    "    return df_slice\n",
    "\n",
    "if(0):\n",
    "    for st in [\"train_set\", \"val_set\"]:\n",
    "        model = pipeline('sentiment-analysis', device=0)\n",
    "        df_chunks = pd.read_csv(os.path.join(PROCESSED_DATASET, f\"{st}.csv\"), chunksize=DATA_CHUNK)\n",
    "        df_list = []\n",
    "        for i, chunk in enumerate(df_chunks):\n",
    "            print(f\"Processing chunk : {i+1} ...\")\n",
    "            curr_sentiments = gen_sentiment(model,chunk)\n",
    "            df_list.append(curr_sentiments) \n",
    "            if (i+1)% 800 ==0:\n",
    "                df_save = pd.concat(df_list)\n",
    "                df_save.to_csv(os.path.join(PROCESSED_DATASET, f\"{st}_labels_{i+1}.csv\"), index=False)\n",
    "        df_save = pd.concat(df_list)\n",
    "        df_save.to_csv(os.path.join(PROCESSED_DATASET, f\"{st}_labels.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set neutral tweets : 0\n",
      "train_set weak positive Tweets : 0\n",
      "train_set strong positive Tweets : 582806\n",
      "train_set weak negative Tweets : 0\n",
      "train_set strong negative Tweets : 1040449\n",
      "val_set neutral tweets : 0\n",
      "val_set weak positive Tweets : 0\n",
      "val_set strong positive Tweets : 193300\n",
      "val_set weak negative Tweets : 0\n",
      "val_set strong negative Tweets : 347601\n"
     ]
    }
   ],
   "source": [
    "### Check the Class distribution of the scores\n",
    "\n",
    "'''\n",
    "Each class number of datapoints. Split the labels into 5 reasons.\n",
    "Strong negative 1.0 to 0.5 and \"NEGATIVE\"\n",
    "Weak negative 0.5 to 0.1 and \"NEGATIVE\"\n",
    "neutral 0.1 - 0 (Negative and Positive both)\n",
    "Weak positive -> 0.1 to 0.5 and Positive\n",
    "'''\n",
    "\n",
    "for st in [\"train_set\", \"val_set\"]:\n",
    "    \n",
    "    df_labels = pd.read_csv(os.path.join(PROCESSED_DATASET, f\"{st}_labels.csv\"))\n",
    "    \n",
    "    neut_ids = (df_labels[\"score\"] > 0.0) & (df_labels[\"score\"]<=0.1)\n",
    "    weak_pos_ids = (df_labels[\"score\"] > 0.1) & (df_labels[\"score\"]<=0.5) & (df_labels[\"label\"] == 1)\n",
    "    strong_pos_ids = (df_labels[\"score\"] > 0.5) & (df_labels[\"score\"]<=1) & (df_labels[\"label\"] == 1)\n",
    "    weak_neg_ids = (df_labels[\"score\"] > 0.1) & (df_labels[\"score\"]<=0.5) & (df_labels[\"label\"]== 0)\n",
    "    strong_neg_ids = (df_labels[\"score\"] > 0.5) & (df_labels[\"score\"]<=1) & (df_labels[\"label\"]== 0)\n",
    "    \n",
    "    print(f\"{st} neutral tweets : {len(df_labels[neut_ids])}\")\n",
    "    print(f\"{st} weak positive Tweets : {len(df_labels[weak_pos_ids])}\")\n",
    "    print(f\"{st} strong positive Tweets : {len(df_labels[strong_pos_ids])}\")\n",
    "    print(f\"{st} weak negative Tweets : {len(df_labels[weak_neg_ids])}\")\n",
    "    print(f\"{st} strong negative Tweets : {len(df_labels[strong_neg_ids])}\")\n",
    "                               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
