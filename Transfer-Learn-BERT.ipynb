{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Transformer based BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING HYPERPARAMETERS\n",
    "PROCESSED_DATASET = \"./processed_dataset\"\n",
    "# Hardcoded after determning thw length due to memory issues\n",
    "MAX_LENGTH = 103\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 2\n",
    "MODEL_DIR = \"./models\"\n",
    "TRAIN_CHUNK = 100000\n",
    "VAL_CHUNK = 20000\n",
    "LRN_RATE = 5e-5\n",
    "# change the argument to \"cpu\" to run on CPU\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A cleanup preprocessing of the text before tokenizing using BERT\n",
    "\n",
    "def text_cleanup(tweet):\n",
    "    tweet = re.sub(r'(@.*?)[\\s]', ' ', tweet)\n",
    "    tweet = re.sub(r'&amp;', '&', tweet)\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_function(data_frame, col_name):\n",
    "    # Attention masks\n",
    "    masks = []\n",
    "    labels = []\n",
    "    ids = []\n",
    "    fixed_length = 0\n",
    "    for idx, tweet in enumerate(data_frame[col_name]):\n",
    "        if type(tweet) is str:\n",
    "            tweet_encoder = bert_tokenizer.encode_plus(text = text_cleanup(tweet), add_special_tokens=True, \n",
    "                                                  return_attention_mask=True, truncation=True, \n",
    "                                                  pad_to_max_length=True)\n",
    "            masks.append(tweet_encoder.get('attention_mask'))\n",
    "            ids.append(tweet_encoder.get('input_ids'))\n",
    "            labels.append(data_frame.iloc[idx,1])\n",
    "    # Pad with fix length\n",
    "    MAX_LENGTH = max([ len(i) for i in ids])\n",
    "    ids = [np.pad(ele, mode= 'constant', pad_width = (0,MAX_LENGTH - len(ele)), constant_values=0).tolist() for ele in ids]\n",
    "    masks = [np.pad(ele, mode= 'constant', pad_width = (0,MAX_LENGTH - len(ele)), constant_values=0).tolist() for ele in masks]\n",
    "        \n",
    "    # Convert masks and ids to torch tensors\n",
    "    return torch.tensor(ids), torch.tensor(masks), torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(df_train_chunk, df_val_chunk):\n",
    "    \n",
    "    train_input_seq, train_mask, train_labels = tokenization_function(df_train_chunk, \"Processed_text\")\n",
    "    val_input_seq, val_mask, val_labels = tokenization_function(df_val_chunk, \"Processed_text\")\n",
    "\n",
    "    \n",
    "    # Training Set Dataloader \n",
    "    tr_tensors = TensorDataset(train_input_seq, train_mask, train_labels)\n",
    "    tr_sampler = RandomSampler(tr_tensors)\n",
    "    tr_dataloader = DataLoader(tr_tensors, sampler=tr_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Validation Set Dataloader \n",
    "    val_tensors = TensorDataset(val_input_seq, val_mask, val_labels)\n",
    "    val_sampler = RandomSampler(val_tensors)\n",
    "    val_dataloader = DataLoader(val_tensors, sampler=val_sampler, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return tr_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment classifier class\n",
    "\n",
    "class Sentiment_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, freeze_model=False):\n",
    "        super(Sentiment_Classifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        dim_in, hidden, dim_out = 768, 50, 2\n",
    "\n",
    "        # Load the pretrained Bert model\n",
    "        self.Bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Feed-Forward Classifier, one layer deep\n",
    "        self.classifier = nn.Sequential(nn.Linear(dim_in, hidden),nn.ReLU(), nn.Linear(hidden, dim_out))\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_model:\n",
    "            for param in self.Bert_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, ids, masks):\n",
    "        \n",
    "        output_vec = self.Bert_model(input_ids=ids,\n",
    "                            attention_mask=masks)\n",
    "        \n",
    "        # classification probs , last hidden state output_vec\n",
    "        class_prob_hidden_state = output_vec[0][:, 0, :]\n",
    "\n",
    "\n",
    "        # Compute logits\n",
    "        logits = self.classifier(class_prob_hidden_state)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_initializer(tr_dataloader):\n",
    "    \n",
    "    # To train keep freeze_model false\n",
    "    classifier = Sentiment_Classifier()\n",
    "    \n",
    "    # Target device\n",
    "    classifier.to(DEVICE)\n",
    "    \n",
    "    # Instantiate the optimizer\n",
    "    classifier_optimizer = AdamW(classifier.parameters(), lr=LRN_RATE, eps=1e-8)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(classifier_optimizer, num_warmup_steps=0, \n",
    "                                                   num_training_steps=EPOCHS*len(tr_dataloader))\n",
    "    \n",
    "    return classifier, classifier_optimizer, lr_scheduler\n",
    "\n",
    "def train(model, tr_loader, val_loader, epochs):\n",
    "    \n",
    "    print(\"Start training...\\n\")\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        acc_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Iterate over each batch and start training\n",
    "        for step, batch in enumerate(tr_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Transfer the batch data to the GPU\n",
    "            batch_ids, batch_masks, batch_labels = tuple(t.to(DEVICE) for t in batch)\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(batch_ids, batch_masks)\n",
    "            # Compute loss and accumulate the loss \n",
    "            loss = loss_func(logits, batch_labels)\n",
    "            batch_loss += loss.item()\n",
    "            acc_loss += loss.item()\n",
    "        \n",
    "            # Back Propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update training parameters\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print training metrics after 500 batches\n",
    "            if ((step+1) % 500 == 0) or ((step+1) == len(tr_dataloader)):\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"Epoch : {epoch + 1} | Step : {step+1} | Train Loss : {batch_loss / batch_counts}\")\n",
    "\n",
    "                # Reset batch parametes\n",
    "                train_loss.append(batch_loss)\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "\n",
    "        # Evaluate\n",
    "        print(f\"Evaluation after {epoch + 1} : \")\n",
    "        avg_val_loss, avg_val_acc = evaluate(model, val_loader)\n",
    "        val_loss.append(avg_val_loss)\n",
    "        val_loss.append(avg_val_acc)\n",
    "                \n",
    "                \n",
    "        print(f\"Saving the model after epoch : {epoch+1}\")\n",
    "        torch.save(model, os.path.join(MODEL_DIR, f\"model_{epoch+1}.pth\"))\n",
    "\n",
    "        #  Average training loss (Entire training set)\n",
    "        avg_train_loss = acc_loss / len(tr_dataloader)\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return train_loss, val_loss, val_acc\n",
    "    \n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    for idx, batch in enumerate(val_loader):\n",
    "        # Load batch to GPU\n",
    "        batch_ids, batch_masks, batch_labels = tuple(t.to(DEVICE) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_ids, batch_masks)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_func(logits, batch_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        pred = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Accuracy\n",
    "        #accuracy = (predictions == batch_labels).cpu().numpy().mean() * 100\n",
    "        acc = (pred == batch_labels).cpu().numpy().mean() * 100\n",
    "        val_acc.append(acc)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    avg_val_loss = np.mean(val_loss)\n",
    "    avg_val_acc = np.mean(val_acc)\n",
    "    \n",
    "    print(f\"The Average validation set val loss : {avg_val_loss}\")\n",
    "    print(f\"The Average validation set val acc : {avg_val_acc}\")\n",
    "    return avg_val_loss, avg_val_acc\n",
    "\n",
    "def predictor(model, data_loader):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    logits_list = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in data_loader:\n",
    "        batch_ids, batch_masks = tuple(t.to(DEVICE) for t in batch)[:2]\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch_ids, batch_masks)\n",
    "        logits_list.append(logits)\n",
    "\n",
    "    # concatenate all the logits\n",
    "    logits_list = torch.cat(logits_list, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(logits_list, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs\n",
    "    \n",
    "\n",
    "def roc_plot(probs, y_labels):\n",
    "    \n",
    "    preds = probs[:, 1]\n",
    "    FPR, TPR, thres = roc_curve(y_labels, preds)\n",
    "    roc_auc = auc(FPR, TPR)\n",
    "    print(f\"AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Plot curve\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(FPR, TPR, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(PROCESSED_DATASET, \"train_set_labels.csv\"), chunksize=TRAIN_CHUNK)\n",
    "df_val = pd.read_csv(os.path.join(PROCESSED_DATASET, \"val_set_labels.csv\"), chunksize=VAL_CHUNK)\n",
    "df_tr_chunks = [chunks for chunks in df_train]\n",
    "df_val_chunks = [chunks for chunks in df_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Epoch : 1 | Step : 500 | Train Loss : 0.4340430860966444\n",
      "Epoch : 1 | Step : 1000 | Train Loss : 0.3549506512284279\n",
      "Epoch : 1 | Step : 1500 | Train Loss : 0.36018231700360775\n",
      "Epoch : 1 | Step : 2000 | Train Loss : 0.3439831671416759\n",
      "Epoch : 1 | Step : 2500 | Train Loss : 0.3558241471201181\n",
      "Epoch : 1 | Step : 3000 | Train Loss : 0.34639341439306737\n",
      "Epoch : 1 | Step : 3500 | Train Loss : 0.3266608180105686\n",
      "Epoch : 1 | Step : 4000 | Train Loss : 0.3480121290385723\n",
      "Epoch : 1 | Step : 4500 | Train Loss : 0.3250024095848203\n",
      "Epoch : 1 | Step : 5000 | Train Loss : 0.3068869077116251\n",
      "Epoch : 1 | Step : 5500 | Train Loss : 0.32014237661659717\n",
      "Epoch : 1 | Step : 6000 | Train Loss : 0.3419086788147688\n",
      "Epoch : 1 | Step : 6500 | Train Loss : 0.32640956791490316\n",
      "Epoch : 1 | Step : 7000 | Train Loss : 0.3222518877983093\n",
      "Epoch : 1 | Step : 7500 | Train Loss : 0.31973231276124714\n",
      "Epoch : 1 | Step : 8000 | Train Loss : 0.30662565389275553\n",
      "Epoch : 1 | Step : 8500 | Train Loss : 0.30838433008641003\n",
      "Epoch : 1 | Step : 9000 | Train Loss : 0.3186307440921664\n",
      "Epoch : 1 | Step : 9500 | Train Loss : 0.31158532917499543\n",
      "Epoch : 1 | Step : 10000 | Train Loss : 0.29617704102396963\n",
      "Epoch : 1 | Step : 10500 | Train Loss : 0.28172518765181304\n",
      "Epoch : 1 | Step : 11000 | Train Loss : 0.2816333241313696\n",
      "Epoch : 1 | Step : 11500 | Train Loss : 0.2868937995880842\n",
      "Epoch : 1 | Step : 12000 | Train Loss : 0.2780288445279002\n",
      "Epoch : 1 | Step : 12496 | Train Loss : 0.2860969454350491\n",
      "Evaluation after 1 : \n",
      "The Average validation set val loss : 0.23606283039599657\n",
      "The Average validation set val acc : 91.2\n",
      "Saving the model after epoch : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sentiment_Classifier. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEmbeddings. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertEncoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfAttention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertSelfOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertIntermediate. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertOutput. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BertPooler. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/amit/.local/lib/python3.6/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      " 50%|█████     | 1/2 [1:22:39<1:22:39, 4959.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 | Step : 500 | Train Loss : 0.21251691346615553\n",
      "Epoch : 2 | Step : 1000 | Train Loss : 0.2118600305840373\n",
      "Epoch : 2 | Step : 1500 | Train Loss : 0.21215352526307105\n",
      "Epoch : 2 | Step : 2000 | Train Loss : 0.22038110157102347\n",
      "Epoch : 2 | Step : 2500 | Train Loss : 0.20431781712919472\n",
      "Epoch : 2 | Step : 3000 | Train Loss : 0.2154346244931221\n",
      "Epoch : 2 | Step : 3500 | Train Loss : 0.21865491896867753\n",
      "Epoch : 2 | Step : 4000 | Train Loss : 0.2173833377957344\n",
      "Epoch : 2 | Step : 4500 | Train Loss : 0.22271438686549663\n",
      "Epoch : 2 | Step : 5000 | Train Loss : 0.19130625396221876\n",
      "Epoch : 2 | Step : 5500 | Train Loss : 0.21073254026472568\n",
      "Epoch : 2 | Step : 6000 | Train Loss : 0.1931838103532791\n",
      "Epoch : 2 | Step : 6500 | Train Loss : 0.21253269450366497\n",
      "Epoch : 2 | Step : 7000 | Train Loss : 0.18592183271795512\n",
      "Epoch : 2 | Step : 7500 | Train Loss : 0.20686367166787387\n",
      "Epoch : 2 | Step : 8000 | Train Loss : 0.21419953181594611\n",
      "Epoch : 2 | Step : 8500 | Train Loss : 0.20509617814421655\n",
      "Epoch : 2 | Step : 9000 | Train Loss : 0.17711374010145664\n",
      "Epoch : 2 | Step : 9500 | Train Loss : 0.1892587732821703\n",
      "Epoch : 2 | Step : 10000 | Train Loss : 0.2000177849829197\n",
      "Epoch : 2 | Step : 10500 | Train Loss : 0.18807110096514226\n",
      "Epoch : 2 | Step : 11000 | Train Loss : 0.1932263864725828\n",
      "Epoch : 2 | Step : 11500 | Train Loss : 0.17030623083561658\n",
      "Epoch : 2 | Step : 12000 | Train Loss : 0.19186730410158634\n",
      "Epoch : 2 | Step : 12496 | Train Loss : 0.17732166764777033\n",
      "Evaluation after 2 : \n",
      "The Average validation set val loss : 0.3047096317276359\n",
      "The Average validation set val acc : 92.48\n",
      "Saving the model after epoch : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [2:45:03<00:00, 4951.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create dataloaders\n",
    "tr_dataloader, val_dataloader = create_dataloader(df_tr_chunks[0], df_val_chunks[0])\n",
    "# Instantiate model class\n",
    "classifier, optimizer, scheduler = model_initializer(tr_dataloader)\n",
    "# train the network\n",
    "train_loss_list, val_loss_list, val_acc_list = train(classifier, tr_dataloader, val_dataloader, EPOCHS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network achieves a 92.48% Validation accuracy with 2 epochs of Fine training. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
